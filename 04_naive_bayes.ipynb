{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT1mFC4NLgnk"
      },
      "source": [
        "### Наивный байесовский классфикатор."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoiNld1yLgnl"
      },
      "source": [
        "Многие из вас могли слышать про [теорему Байеса](http://mathprofi.ru/formula_polnoj_verojatnosti_formuly_bajesa.html) из курса матстата и теории вероятностей:\n",
        "\n",
        "$\\large P(A_i|B) = \\frac{P(A_iB)}{P(B)} =  \\frac{P(B \\mid A_i)\\, P(A_i)}{P(B)}$\n",
        "\n",
        "Кратко опишем смысл формулы:  \n",
        "\n",
        "_Пусть некое событие $B$ может наступить в результате осуществеления одной из гипотез $A_1$, $A_2$, $A_3$ и тд.  \n",
        "Зная вероятности гипотез $A$ до наступления события, можно, уже после свершения события $B$, вычислить, какая из гипотез привела к свершению этого события с наибольшей вероятностью._\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HFKle3KLgnn"
      },
      "source": [
        "В примере выше у нас был только один признак (курение), но что делать, если признаков больше одного?  \n",
        "\n",
        "Формула изменится таким образом:\n",
        "\n",
        "$ P(A_i|B_1, B_2, ... , B_n) = \\frac{P(B_1, B_2, ... , B_n | A_i)*P(A_i)}{P(B)}$\n",
        "\n",
        "Условная вероятность в числителе расписывается на произведение вероятностей. Тут есть важный нюанс - мы наивно предполагаем, что признаки $B_1$, $B_2$, ... , $B_n$ независимы друг от друга, то есть они никак не коррелируют друг с другом:\n",
        "\n",
        "$ P(B_1, B_2, ... , B_n | A_i) = P(B_1 | A_i) * P(B_2 | A_i) *\\ ...\\ * P(B_n | A_i) $\n",
        "\n",
        "Значит, перед использованием классификатора в идеале необходимо проверить все признаки на взаимную корреляцию, и удалить сильно коррелирующие. Для поиска можно использовать [критерий Пирсона](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb-fUZMSLgnm"
      },
      "source": [
        "Априорные и апостериорные вероятности и так далее - это конечно интересно, но где тут связь с классификацией?! Вспомним типичную задачку на теорему байеса:  \n",
        "\n",
        "**_В выборке 40% мужчин и 60% женщин. Известно, что среди них курит 10% женщин и 70% мужчин. Про некоего человека N известно, что он курит. Мужчина он, или женщина?_**\n",
        "\n",
        "Данная задачка по сути есть классификация - у нас есть значение нецелевого признака (курение), и на основе этого нам надо предсказать пол. Решение задачки очень простое, так как все нужные нам числа прописаны сразу в условии:\n",
        "\n",
        "$P(M) = 0.4$  \n",
        "$P(Ж) = 0.6$  \n",
        "$P(K|M) = 0.7$ - вероятность того, что человек курит (К) если он мужчина.  \n",
        "$P(К|Ж) = 0.1$ - вероятность того, что человек курит, если он женщина.  \n",
        "\n",
        "Понятно, что является человек мужчиной или женщиной - это гипотезы, а вероятность того, что курит или не курит - событие, которое может произойти в результате наступления этих гипотез.  \n",
        "Рассчитаем полную вероятность того, что человек курит:\n",
        "$P(К) = P(M)*P(K|M) + P(Ж)*P(К|Ж) = 0.34$.\n",
        "\n",
        "Теперь мы сможем наконец предсказать пол человека зная, курит он (она?) или нет:\n",
        "\n",
        "$P(M|К) = \\frac{P(К|М)*P(M)}{P(К)} = 28/34$\n",
        "\n",
        "$P(Ж|К) = \\frac{P(К|Ж)*P(Ж)}{P(К)} = 6/34$\n",
        "\n",
        "Видно, что тут у нас есть вероятности всех классов. Выбираем просто класс с наибольшей вероятностью (argmax) - и это мужчина. Очевидный плюс такого классифкатора - не требуется большой объем данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PGX5TFJLgno"
      },
      "source": [
        "Любопытно, что если реализовывать классификатор \"в лоб\", по определению теоремы байеса, то точность его будет не высока, так как не учитывается то, каким образом распределены сами данные. Подробно про разные варианты байесовского классификатора можно почитать [тут](https://scikit-learn.org/stable/modules/naive_bayes.html). В данной работе мы будем реализовывать [несколько упрощенный мультиномиальный классификатор](http://bazhenov.me/blog/2012/06/11/naive-bayes.html). Настоятельно рекомендую **прочесть пример** по ссылке и разобраться, каким образом высчитываются все параметры. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51JEID56Lgno"
      },
      "source": [
        "### Пред-подготовка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-CH-x9yLgnp"
      },
      "source": [
        "Для начала мы разберемся с данными, а затем реализуем классификатор.\n",
        "\n",
        "Говоря о самих данных, то мы попробуем решить задачу классификации [новостных текстов](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) - байесовские классификаторы часто используются в этой прикладной области машинного обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANyLqj0XLgnp",
        "outputId": "f1f9fcce-a934-43ef-8b9b-d88fd5cac2d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# посмотрим на данные - в этот раз датасет доступен напрямую из sklearn.\n",
        "# выведем все доступные категории.\n",
        "# параметр subset отвечает за разделенение данных (train - тренировочная выборка, test - тестовая).\n",
        "\n",
        "# параметр remove говорит о том, какие части данных нужно удалить, чтобы не допустить переобучения.\n",
        "# headers - заголовки новостных групп\n",
        "# quotes - удаление строк, похожих на цитаты из других источников\n",
        "# footers - удаление блоков из конца текста, похожих на подписи\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_train.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZengtNm_Lgns"
      },
      "source": [
        "Мы будем использовать только 4 класса текстов: `alt.atheism`, `sci.space`, `talk.religion.misc`, `comp.graphics`.  Используя параметр `categories` в функции `fetch_20newsgroups`, задайте список нужных нам категорий и разбейте данные на тренировочную и тестовые части (параметр `subset`). \n",
        "\n",
        "Учтите, что сами данные (целевые и нецелевые признаки) лежат в атрибутах `data` и `target`:\n",
        "```python\n",
        "subset = fetch_20newsgroups( ... )\n",
        "X = subset.data\n",
        "y = subset.target\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CEN5cAq3Lgns"
      },
      "outputs": [],
      "source": [
        "categories = ('alt.atheism', 'sci.space', 'talk.religion.misc', 'comp.graphics')\n",
        "\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    remove=('headers', 'footers', 'quotes'),\n",
        "    categories = categories)\n",
        "\n",
        "newsgroups_test = fetch_20newsgroups(\n",
        "    subset='test',\n",
        "    remove=('headers', 'footers', 'quotes'),\n",
        "    categories = categories)\n",
        "\n",
        "x_train = newsgroups_train.data\n",
        "x_test = newsgroups_test.data\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_GZP6zJLgnt"
      },
      "source": [
        "Посмотрим на типы данных: видно, что X - это список со строками, а y - просто массив с метками класса."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CGx11VSLgnt",
        "outputId": "610e7120-8097-47d4-c9f1-604be921c358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "<class 'str'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.int64'>\n"
          ]
        }
      ],
      "source": [
        "print(type(x_train))\n",
        "print(type(x_train[0]))\n",
        "print(type(y_train))\n",
        "print(type(y_train[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3mGN3_ALgnu"
      },
      "source": [
        "Выведите на экран по 1 тексту из каждой категории."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PZTBntG4Lgnu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Категория: alt.atheism\n",
            "==================================================\n",
            "I have a request for those who would like to see Charley Wingate\n",
            "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
            "appear to be quite a few of you.)  \n",
            "\n",
            "It is clear that Mr. Wingate intends to continue to post tangential or\n",
            "unrelated articles while ingoring the Challenges themselves.  Between\n",
            "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
            "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
            "\n",
            "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
            "will just go away, and he is doing his level best to change the\n",
            "subject.  Given that this seems a rather common net.theist tactic, I\n",
            "would like to suggest that we impress upon him our desire for answers,\n",
            "in the following manner:\n",
            "\n",
            "1. Ignore any future articles by Mr. Wingate that do not address the\n",
            "Challenges, until he answers them or explictly announces that he\n",
            "refuses to do so.\n",
            "\n",
            "--or--\n",
            "\n",
            "2. If you must respond to one of his articles, include within it\n",
            "something similar to the following:\n",
            "\n",
            "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
            "\n",
            "Really, I'm not looking to humiliate anyone here, I just want some\n",
            "honest answers.  You wouldn't think that honesty would be too much to\n",
            "ask from a devout Christian, would you?  \n",
            "\n",
            "Nevermind, that was a rhetorical question.\n",
            "\n",
            "==================================================\n",
            "\n",
            "Категория: comp.graphics\n",
            "==================================================\n",
            "Hi,\n",
            "\n",
            "I've noticed that if you only save a model (with all your mapping planes\n",
            "positioned carefully) to a .3DS file that when you reload it after restarting\n",
            "3DS, they are given a default position and orientation.  But if you save\n",
            "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
            "know why this information is not stored in the .3DS file?  Nothing is\n",
            "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
            "I'd like to be able to read the texture rule information, does anyone have \n",
            "the format for the .PRJ file?\n",
            "\n",
            "Is the .CEL file format available from somewhere?\n",
            "\n",
            "Rych\n",
            "\n",
            "==================================================\n",
            "\n",
            "Категория: sci.space\n",
            "==================================================\n",
            "\n",
            " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
            "\n",
            "MB>                                                             So the\n",
            "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
            "\n",
            "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
            "\n",
            "Couldn't we just say periapsis or apoapsis?\n",
            "\n",
            " \n",
            "\n",
            "==================================================\n",
            "\n",
            "Категория: talk.religion.misc\n",
            "==================================================\n",
            "\n",
            "\n",
            "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
            "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
            "folks with him, children and all, to satisfy his delusional mania. Jim\n",
            "Jones, circa 1993.\n",
            "\n",
            "\n",
            "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
            "for centuries.\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for category_idx, category_name in enumerate(newsgroups_train.target_names):\n",
        "    print(f\"Категория: {category_name}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Находим индекс первого текста в текущей категории\n",
        "    first_text_idx = next(i for i, label in enumerate(newsgroups_train.target) if label == category_idx)\n",
        "\n",
        "    # Выводим текст\n",
        "    print(newsgroups_train.data[first_text_idx])\n",
        "    print(\"\\n\" + \"=\" * 50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q107gR1lLgnv"
      },
      "source": [
        "проведем небольшой эксперимент по отбору признаков: датасет изкоробочный, специально для обучения машинному обучению (sic), НО...  \n",
        "... проверим данные на наличие пробелов и пустых строк"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk59sXkXLgnv",
        "outputId": "d1dea159-59bb-45fb-9c57-c3262eec5f88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47\n",
            "4\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(x_train.count(''))\n",
        "print(x_train.count(' '))\n",
        "print(x_train.count('  '))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8CxsEhrLgnw"
      },
      "source": [
        "Как мы видим, среди признаков внезапно оказались пустые строки, и этим пустым строкам присвоен класс! Очевидно, что такое недопустимо, поэтому датасет необходимо немного вычистить.  \n",
        "\n",
        "Заметьте, что мы делаем проверку на строку с 1м, 2мя, 3мя пробелами, но не с 5 пробелами и больше. Чтобы эффективно найти строки с некоторым неизвестным числом пробелов, чтоит воспользоваться регулярным выражением `^\\\\s*$` - данная регулярка срабатывает на строках, состоящих целиком из пробелов (`\\s` - это пробельный символ, квантификатор `*` указывает, что число повторений такого символа больше одного, символ `^` указывает на начало строки, а знак доллара- на конец строки).\n",
        "\n",
        "Для нахождения возьмем функцию `match` из библиотеки `re` регулярных выражений в питоне.  \n",
        "Доки по функции [match](https://docs.python.org/3/library/re.html#re.match). Обратите внимание, что она возвратит `None`, если паттерн не совпал с заданной строкой, или возвратит некий `match object`, если будет совпадение.  \n",
        "\n",
        "**Задача:** в тестовой и тренировочной выборках найти индексы пробельных строк. Зная индексы (это должен быть массив индексов), можно удалить такие элементы из тренировочной и тестовой выборок. Для удаления можете использовать логические маски, или [np.delete](https://numpy.org/doc/stable/reference/generated/numpy.delete.html)\n",
        "\n",
        "**Hints**\n",
        "- np.delete\n",
        "- re.match\n",
        "- '''^\\\\s*$'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly0SwGwkLgnw"
      },
      "source": [
        "Напечатаем число элементов до очистки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im9ZV2OZLgnw",
        "outputId": "e4ed0d2d-7c4f-4921-b298-db83107ab501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2034\n",
            "1353\n"
          ]
        }
      ],
      "source": [
        "print(len(y_train))\n",
        "print(len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VsPBJdA3Lgnx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
            "C:\\Temp\\ipykernel_13772\\3227690409.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  pattern = re.compile('^\\s*$')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Удалено 57 пробельных строк из тренировочной выборки.\n",
            "Удалено 35 пробельных строк из тестовой выборки.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np \n",
        "\n",
        "pattern = re.compile('^\\s*$')\n",
        "\n",
        "# Функция для поиска индексов пробельных строк\n",
        "def find_empty_indices(data):\n",
        "    empty_indices = [i for i, text in enumerate(data) if pattern.match(text)]\n",
        "    return empty_indices\n",
        "\n",
        "# Находим индексы пробельных строк в тренировочной и тестовой выборках\n",
        "empty_indices_train = find_empty_indices(x_train)\n",
        "empty_indices_test = find_empty_indices(x_test)\n",
        "\n",
        "# Удаляем пробельные строки из тренировочной и тестовой выборок\n",
        "x_train_cleaned = np.delete(x_train, empty_indices_train)\n",
        "y_train_cleaned = np.delete(y_train, empty_indices_train)\n",
        "\n",
        "x_test_cleaned = np.delete(x_test, empty_indices_test)\n",
        "y_test_cleaned = np.delete(y_test, empty_indices_test)\n",
        "\n",
        "# Проверка результатов\n",
        "print(f\"Удалено {len(empty_indices_train)} пробельных строк из тренировочной выборки.\")\n",
        "print(f\"Удалено {len(empty_indices_test)} пробельных строк из тестовой выборки.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFfVJ7G9Lgny"
      },
      "source": [
        "Выведем число элементов после очистки, должно получиться 1977 и 1318 элементов в тренировочной и тестовой выборках:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Fagvq0cDLgny"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер x_train после очистки: 1977\n",
            "Размер y_train после очистки: 1977\n",
            "Размер x_test после очистки: 1318\n",
            "Размер y_test после очистки: 1318\n"
          ]
        }
      ],
      "source": [
        "# Проверка размеров данных после очистки\n",
        "print(f\"Размер x_train после очистки: {len(x_train_cleaned)}\")\n",
        "print(f\"Размер y_train после очистки: {len(y_train_cleaned)}\")\n",
        "print(f\"Размер x_test после очистки: {len(x_test_cleaned)}\")\n",
        "print(f\"Размер y_test после очистки: {len(y_test_cleaned)}\")\n",
        "\n",
        "# Обновление утверждений\n",
        "assert len(y_train_cleaned) == len(x_train_cleaned), \"Размеры x_train и y_train не совпадают\"\n",
        "assert len(y_test_cleaned) == len(x_test_cleaned), \"Размеры x_test и y_test не совпадают\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7Ce0CS80QNO"
      },
      "source": [
        "Посмотрим на то, что из себя представляют целевые признаки. Это целое число, обозначающее индекс категории."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGxd_WiU0NkG",
        "outputId": "7b1433b3-d5b3-41ce-da90-5deeb947d909"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3], dtype=int64)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbacr8Dh0olZ"
      },
      "source": [
        "Преобразуем этот индекс в имя категории. Для этого воспользуемся генератором списков и методом `target_names`, который по индексу вернет нам название категории."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3sUndLwf0oDE"
      },
      "outputs": [],
      "source": [
        "y_test = [newsgroups_test.target_names[idx] for idx in y_test]\n",
        "y_train = [newsgroups_train.target_names[idx] for idx in y_train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vaYpBsX2ADT",
        "outputId": "9942dc58-7c80-40d6-ccb0-00014ade5dfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc'],\n",
              "      dtype='<U18')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2qqYzkLLgnz"
      },
      "source": [
        "### Как извлечь информацию из текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWsJ4rDvLgnz"
      },
      "source": [
        "В предыдущей части вы подготовили сам датасет, состоящий из строк с символами. Но каким образом конвертировать символы в числа, чтобы предложение образовало точку в некотором многомерном пространстве?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjV9C_21Lgnz"
      },
      "source": [
        "Для этого есть несколько способов:\n",
        "- Счетчики\n",
        "- Распределенная семантика\n",
        "\n",
        "**Счетчики** работают довольно просто - из слов всех предложений выборки вы формируете один большой словарь, и, получая новое предложение, просто считаете, сколько раз каждое слово из словаря было представлено в этом предложении. Таким образом, ваше предложение закодировано вектором, длина которого равна длине словаря. Недостатков у этого метода только два: получающийся вектор имеет очень большую длину, и к тому же он сильно разряжен (в нем много нулей, так как одно предложение априори не может содержать всех слов), из-за чего с ним трудно работать. Второй недостаток - мы просто смотрим на сам _факт наличия_ слова в предложении, но не на _контекст_ этого слова.\n",
        "\n",
        "Оба этих недостатка успешно забарывают методы, основанные на **распределенной семантике**, такие как легендарный Word2Vec и известный GloVe. Такие методы основаны на недо-нейронных сетях, поволяют учитывать контекст слова, и позволяют создавать вектора заданной пользователем длины. \n",
        "\n",
        "Проблема их в том, что значения таких векторов - это и отрицательные числа, а байесовская модель работает только с натуральными. К тому же, такие алгоритмы работают на уровне слова, а не предложения - у вас просто будет набор векторов, представляющих каждое слово из предложения, а каким образом представить вектор самого предложения с наименьшими потерями информации - вопрос.  Поэтому далее мы будем использовать именно счетчики."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7ifetN0Lgn0"
      },
      "source": [
        "Для работы со счетчиками мы возьмем реализацию из библиотеки `sklearn`.  \n",
        "\n",
        "по ссылке [тут](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) можно почитать про сами счетчики (мешок слов и TF-IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KvV7CvyLgn1"
      },
      "source": [
        "#### **Мешок слов**\n",
        "\n",
        "[Документация](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)\n",
        "\n",
        "Можно начать с очень простой идеи. Давайте разобъем все предложения на слова. Составим словарь всех слов, которые будут встречаться во всех  наших текстах. И отметим, встречается ли это слово в нашем конкретном примере. Другими словами, пусть в таблице в строках будут предложения, в столбцах - слова, а в ячейках число, которое показывает сколько раз это слово встречалось в этом предложении. Получается, что каждому объекту выборки будет сопоставлен вектор.\n",
        "\n",
        "Векторизацию мы делаем сразу методом `fit_transform` - он эквивалентен последовательному вызову \n",
        "```python\n",
        "bow = count_vectorizer.fit(data).transform(data)\n",
        "```\n",
        "\n",
        "Очевидно, что метод `fit` составляет словарь, а `transform` делает вектор из предложения, согласно имеющемуся словарю."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVvPgtemLgn1",
        "outputId": "33b56c28-f9fc-4cfc-852b-8c9f2e4e4a92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape= (3, 28)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "texts = [\n",
        "    \"I've been searching for the right words to thank you for this breather.\",\n",
        "    \"You have been wonderful and a blessing at all times\",\n",
        "    \"I promise i wont take your help for granted and will fulfil my promise.\"\n",
        "]\n",
        "bow = count_vectorizer.fit_transform(texts)\n",
        "print(\"Shape=\", bow.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFaq7qzZLgn2",
        "outputId": "64ca588e-f353-4bb2-df3f-9b983076db2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ve': 21,\n",
              " 'been': 3,\n",
              " 'searching': 14,\n",
              " 'for': 6,\n",
              " 'the': 17,\n",
              " 'right': 13,\n",
              " 'words': 25,\n",
              " 'to': 20,\n",
              " 'thank': 16,\n",
              " 'you': 26,\n",
              " 'this': 18,\n",
              " 'breather': 5,\n",
              " 'have': 9,\n",
              " 'wonderful': 23,\n",
              " 'and': 1,\n",
              " 'blessing': 4,\n",
              " 'at': 2,\n",
              " 'all': 0,\n",
              " 'times': 19,\n",
              " 'promise': 12,\n",
              " 'wont': 24,\n",
              " 'take': 15,\n",
              " 'your': 27,\n",
              " 'help': 10,\n",
              " 'granted': 8,\n",
              " 'will': 22,\n",
              " 'fulfil': 7,\n",
              " 'my': 11}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# посмотрим на словарь всех слов (метод vocabulary_)\n",
        "# число - это индекс слова в строке матрицы\n",
        "\n",
        "count_vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPPj1XurLgn3"
      },
      "source": [
        "Теперь составим ту самую матрицу, где в столбцах слова, а в строках тексты.\n",
        "\n",
        "Как мы видим, в первом и втором предложениях есть слово \"been\", а в третьем его нет (так как у \"been\" индекс равен 3).  \n",
        "Так как векторайзер возвращает разряженную матрицу, то воспользуемся методом `.toarray()`, чтобы превратить ее в numpy-массив."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03EpcKYYLgn4",
        "outputId": "eaab7578-fada-45b1-b2a7-39cb34cabd1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
              "        0, 0, 0, 1, 1, 0],\n",
              "       [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "        0, 1, 0, 0, 1, 0],\n",
              "       [0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        1, 0, 1, 0, 0, 1]], dtype=int64)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bow.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ULPT499Lgn4"
      },
      "source": [
        "При векторизации можно удалить \"стоп-слова\" - они не несут какого-то смысла, но нужны для грамматики (параметр `stop_words`). Как мы видим, словарь стал заметно меньше, соответсвенно и вектор тоже стал короче."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZWkXFwYLgn4",
        "outputId": "2201c1fb-ce0b-4a0d-a8ea-a51fd09b3668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape= (3, 14)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ve': 10,\n",
              " 'searching': 7,\n",
              " 'right': 6,\n",
              " 'words': 13,\n",
              " 'thank': 8,\n",
              " 'breather': 1,\n",
              " 'wonderful': 11,\n",
              " 'blessing': 0,\n",
              " 'times': 9,\n",
              " 'promise': 5,\n",
              " 'wont': 12,\n",
              " 'help': 4,\n",
              " 'granted': 3,\n",
              " 'fulfil': 2}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "bow = count_vectorizer.fit_transform(texts)\n",
        "print(\"Shape=\", bow.shape)\n",
        "count_vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H77xYv43Lgn5"
      },
      "source": [
        "#### **TF-IDF**\n",
        "\n",
        "[Документация в sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
        "\n",
        "Мешок слов не учитывает \"веса\" слов, он просто смотрит их вхождение в документ. Вероятно, было бы полезно взвесить каким-то образом каждое слово в документе. Действительно, если слово встречается во всех документах, то, наверное, его вес небольшой. А если редкое слово встречается в некоторых документах, то скорее всего оно какое-то узко тематическое.\n",
        "\n",
        "Один из способов взвесить слова - это использовать меру tf-idf, где:\n",
        "\n",
        "**TF - term frequency** - частота слова для каждой статьи\n",
        "\n",
        "$$\\LARGE \\mathrm{tf}(t,d) = \\frac{n_t}{\\sum_k n_k}$$\n",
        "\n",
        "**IDF - inverse document frequency*** — обратная частота документа - уменьшает вес часто встречаемых слов\n",
        "\n",
        "$$\\LARGE \\mathrm{idf}(t, D) =  \\log \\frac{|D|}{|\\{\\,d_i \\in D \\mid t \\in d_{i}\\, \\}|}$$\n",
        "\n",
        "$|D|$ - число документов в корпусе\n",
        "\n",
        "$|\\{\\,d_i \\in D \\mid t \\in d_{i}\\, \\}|$ - число документов из коллекции ${\\displaystyle D}$ , в которых встречается ${\\displaystyle t}$  (когда ${\\displaystyle n_{t}\\neq 0}$ ).\n",
        "\n",
        "**TF-IDF**\n",
        "\n",
        "$$\\LARGE \\operatorname{tf-idf}(t,d,D) = \\operatorname{tf}(t,d) \\times \\operatorname{idf}(t, D)$$\n",
        "\n",
        "\n",
        "Синтаксис такой же, как и у мешка слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6uWHcobLgn5",
        "outputId": "6ec16849-1509-4507-f41c-1c9df3460a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape= (3, 14)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "texts = [\n",
        "    \"I've been searching for the right words to thank you for this breather.\",\n",
        "    \"You have been wonderful and a blessing at all times\",\n",
        "    \"I promise i wont take your help for granted and will fulfil my promise.\"\n",
        "]\n",
        "bow = tfidf_vectorizer.fit_transform(texts)\n",
        "print(\"Shape=\", bow.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDwWgta3Lgn6",
        "outputId": "5f1ddce9-0ca9-484f-e874-2b4bf8cff4ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ve': 10,\n",
              " 'searching': 7,\n",
              " 'right': 6,\n",
              " 'words': 13,\n",
              " 'thank': 8,\n",
              " 'breather': 1,\n",
              " 'wonderful': 11,\n",
              " 'blessing': 0,\n",
              " 'times': 9,\n",
              " 'promise': 5,\n",
              " 'wont': 12,\n",
              " 'help': 4,\n",
              " 'granted': 3,\n",
              " 'fulfil': 2}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j23wuTh9Lgn6",
        "outputId": "d2d4956a-a3db-418d-e5b8-e7c70fb505ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.        , 0.40824829, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.40824829, 0.40824829, 0.40824829, 0.        ,\n",
              "        0.40824829, 0.        , 0.        , 0.40824829],\n",
              "       [0.57735027, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
              "        0.        , 0.57735027, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.35355339, 0.35355339, 0.35355339,\n",
              "        0.70710678, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.35355339, 0.        ]])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bow.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khpVDEVgLgn6"
      },
      "source": [
        "#### **Задание**\n",
        "\n",
        "Обратясь к примерам выше и к документации по ссылкам, создайте векторайзеры для подготовленных в предыдущем параграфе данных. Используйте английские стоп-слова. \n",
        "\n",
        "Так как у нас есть две части (тренировочная и тестовая выборки), то векторайзер нужно обучить на словах из обоих выборок (подумайте, почему). Так как у нас питон, а наши выборки это массивы строк, то объеденить их очень просто - просто сложить. После того, как вы обучили векторайзер на всех словах, проведите трансформации отдельно для тестовой, и отдельно для тренировочных частей.\n",
        "\n",
        "Векторизованные части **назовите** `xcv_test/train` для count_vectorizer, и `xTfidf_test/train` для TF-IDF - это нужно для корректной работы тестов и примеров ниже."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "roxobqNsLgn7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CountVectorizer:\n",
            "xcv_train shape: (1977, 33529)\n",
            "xcv_test shape: (1318, 33529)\n",
            "\n",
            "TfidfVectorizer:\n",
            "xTfidf_train shape: (1977, 33529)\n",
            "xTfidf_test shape: (1318, 33529)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Объединяем тренировочную и тестовую выборки\n",
        "all_texts = np.concatenate((x_train_cleaned, x_test_cleaned))\n",
        "\n",
        "# Создаём и обучаем CountVectorizer\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "count_vectorizer.fit(all_texts)\n",
        "\n",
        "# Преобразуем тренировочную и тестовую выборки\n",
        "xcv_train = count_vectorizer.transform(x_train_cleaned)\n",
        "xcv_test = count_vectorizer.transform(x_test_cleaned)\n",
        "\n",
        "# Создаём и обучаем TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_vectorizer.fit(all_texts)\n",
        "\n",
        "# Преобразуем тренировочную и тестовую выборки\n",
        "xTfidf_train = tfidf_vectorizer.transform(x_train_cleaned)\n",
        "xTfidf_test = tfidf_vectorizer.transform(x_test_cleaned)\n",
        "\n",
        "# Проверка размеров\n",
        "print(\"CountVectorizer:\")\n",
        "print(f\"xcv_train shape: {xcv_train.shape}\")\n",
        "print(f\"xcv_test shape: {xcv_test.shape}\")\n",
        "\n",
        "print(\"\\nTfidfVectorizer:\")\n",
        "print(f\"xTfidf_train shape: {xTfidf_train.shape}\")\n",
        "print(f\"xTfidf_test shape: {xTfidf_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9llJqtHLgn7"
      },
      "source": [
        "Небольшой тест на проверку размерностей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "phJ24tZqLgn7"
      },
      "outputs": [],
      "source": [
        "# count vectorizer\n",
        "assert xcv_train.shape == (1977, 33529)\n",
        "assert xcv_test.shape == (1318, 33529)\n",
        "\n",
        "#tf-idf\n",
        "assert xTfidf_train.shape == (1977, 33529)\n",
        "assert xTfidf_test.shape == (1318, 33529)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JAwiOt_Lgn7"
      },
      "source": [
        "<a name=\"clf-description\"></a>\n",
        "### Реализация классификатора\n",
        "\n",
        "Вспомните статью из блога, которая была в самом начале.\n",
        "\n",
        "Модель классификатора строится на основе обучающей выборки. \n",
        "По ней необходимо найти следующюю статистику:  \n",
        "1. Частоты классов в корпусе объектов (сколько объектов принадлежит каждому из классов)  (`classes_stats`)\n",
        "2. Cуммарное число слов в документах каждого класса (`words_per_class`, далее см. $L_c$)\n",
        "3. Частоты слов в пределах каждого класса (`word_freqs_per_class`, далее используется для расчета $W_{ic}$)\n",
        "4. Размер словаря выборки (число признаков) - кол-во уникальных слов в выборке (`num_features`)\n",
        "\n",
        "По сути, это метод `fit` классификатора.\n",
        "\n",
        "На этапе предсказания необходимо воспользоваться следующей формулой:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "predicted\\ class = \\operatorname*{arg\\max}_{c \\in C} \\left[\\log{{D_c} \\over {D}} + \\sum_{i \\in Q}{\\log{{W_{ic} + 1} \\over {|V| + L_c}}}\\right]\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "Поясним некоторые переменные в этом выражении:  \n",
        "$D_c$ - количество документов в обуч. выборке $\\in$ классу $c$\n",
        "\n",
        "$D$ - сколько всего было документов в обуч. выборке\n",
        "\n",
        "$|V|$ - количество уникальных слов во сех документах обуч. выборки\n",
        "\n",
        "$L_c$ - cуммарное число слов в документах класса $c$ обучающей выборки\n",
        " \n",
        "$W_{ic}$ - сколько раз $i$ слово встретилось в объектах класса $c$ обучающей выборки\n",
        "\n",
        "$Q$ - множество слов _классифицируемого_ документа\n",
        "\n",
        "Сигнатура класса:\n",
        "\n",
        "```python\n",
        "class NaiveBayes:\n",
        "    def fit(self, x, y) -> None\n",
        "    \n",
        "    def predict(self, x) -> List[Int]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOl06mCYLgn8"
      },
      "source": [
        "Для начала отдельно подсчитаем различные статистики, описанные в статье и в материале выше. \n",
        "Так как у нас уже готовы все данные, то считать будем по **count_vectorizer**'у (то есть xcv_ ...)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xit7_y8jLgn8"
      },
      "source": [
        "Общее число документов в обучающей выборке (`doc_num`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "aHPsWi3eLgn8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1977\n"
          ]
        }
      ],
      "source": [
        "doc_num = x_train_cleaned.shape[0]\n",
        "print(doc_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "i6rJBGiKaU-7"
      },
      "outputs": [],
      "source": [
        "# ПРОВЕРКА\n",
        "assert doc_num == 1977"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZdjDDTELgn8"
      },
      "source": [
        "Словарь, содержащий число объектов каждого класса (`classes_stats`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "APfAvqcyLgn9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Статистика по классам в тренировочной выборке: {'comp.graphics': 571, 'talk.religion.misc': 361, 'sci.space': 577, 'alt.atheism': 468}\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Создаем словарь с количеством объектов каждого класса в тренировочной выборке\n",
        "class_indices = y_train_cleaned\n",
        "class_names = newsgroups_train.target_names\n",
        "\n",
        "# Считаем количество объектов каждого класса\n",
        "class_counts = Counter(class_indices)\n",
        "\n",
        "# Создаем словарь, где ключи — имена классов, а значения — количество объектов\n",
        "classes_stats = {class_names[idx]: count for idx, count in class_counts.items()}\n",
        "\n",
        "# Выводим результат\n",
        "print(\"Статистика по классам в тренировочной выборке:\", classes_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vkyU6KypajSC"
      },
      "outputs": [],
      "source": [
        "# ПРОВЕРКА\n",
        "assert classes_stats['alt.atheism'] == 468\n",
        "assert classes_stats['comp.graphics'] == 571\n",
        "assert classes_stats['sci.space'] == 577\n",
        "assert classes_stats['talk.religion.misc'] == 361"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMPXQjAGLgn9"
      },
      "source": [
        "Число уникальных признаков (слов) в тренировочной выборке (`num_features`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fbsLa0wiLgn9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Число уникальных признаков (слов) в тренировочной выборке: 33529\n"
          ]
        }
      ],
      "source": [
        "num_features = xcv_train.shape[1]\n",
        "\n",
        "# Вывод результата\n",
        "print(\"Число уникальных признаков (слов) в тренировочной выборке:\", num_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "r1PgsyaHanzE"
      },
      "outputs": [],
      "source": [
        "# ПРОВЕРКА\n",
        "assert num_features == 33529"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7kZMoe1Lgn9"
      },
      "source": [
        "Создадим словарь `indexes`, в котором ключом будет являться имя класса, а значением - список строк матрицы X, принадлежащих этому классу. Этот список пригодится нам дальше, так как будет играть роль маски. Для поиска класса каждой из строк используйте целевой вектор.\n",
        "\n",
        "**Hints:**\n",
        "- [np.where](https://numpy.org/doc/stable/reference/generated/numpy.where.html) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bbO1neuVLgn-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество строк для класса 'alt.atheism': 468\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Создаём словарь indexes\n",
        "indexes = {}\n",
        "\n",
        "# Проходим по всем уникальным классам\n",
        "for category_idx, category_name in enumerate(newsgroups_train.target_names):\n",
        "    # Находим индексы строк, принадлежащих текущему классу\n",
        "    class_indices = np.where(y_train_cleaned == category_idx)[0]\n",
        "    \n",
        "    # Добавляем в словарь\n",
        "    indexes[category_name] = class_indices\n",
        "\n",
        "# Пример использования: выбор строк для класса 'alt.atheism'\n",
        "atheism_indices = indexes['alt.atheism']\n",
        "print(f\"Количество строк для класса 'alt.atheism': {len(atheism_indices)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HmjQZiKLauNb"
      },
      "outputs": [],
      "source": [
        "# ПРОВЕРКА\n",
        "# так как в словаре очень много элементов, то проверим случайные элементы из списка.\n",
        "# если вы все сделали правильно, то эти элементы совпадут.\n",
        "assert indexes['sci.space'][35] == 111\n",
        "assert indexes['comp.graphics'][42] == 159\n",
        "assert indexes['talk.religion.misc'][67] == 312\n",
        "assert indexes['alt.atheism'][89] == 372"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipTcV_I1Lgn-"
      },
      "source": [
        "Используя найденные выше индексы, подсчитаем два важных параметра `words_per_class` и `word_freqs_per_class`.  \n",
        "Обе этих переменных являются словарями, но первая из них отвечает за суммарное число слов, использованных в **каждом классе**, а вторая показывает, сколько раз конкретное слово встретилось в документах определенного класса.  Соответственно, формат переменной `words_per_class` - `{str: int}`, формат `word_freqs_per_class` - `{str: List}`.\n",
        "Мы специально объеденили поиск двух разных статистик в одном блоке, чтобы избежать лишних циклов.\n",
        "\n",
        "Чтобы найти в X строки, относящиеся к тому или иному классу, воспользуйтесь поиском по маске `indexes` для нужного класса.\n",
        "\n",
        "Также помните, что X - это разряженная матрица, но из нее можно получить обычный список через метод `toarray()`\n",
        "\n",
        "**Hints::**\n",
        "- вспомните про маски в numpy-массивах\n",
        "```python\n",
        "mask = [1,0,2] #indexes\n",
        "array = [1,2,4,8,16,32,64,128]\n",
        "array[mask]\n",
        "#result: array([2, 1, 4])\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHxuZaP9Lgn-",
        "outputId": "d94c9aeb-7193-4211-e38e-4651f27e9872"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'alt.atheism': 8737,\n",
              "  'comp.graphics': 10592,\n",
              "  'sci.space': 13273,\n",
              "  'talk.religion.misc': 8860},\n",
              " {'alt.atheism': array([ 0, 17,  0, ...,  0,  0,  0], dtype=int64),\n",
              "  'comp.graphics': array([26, 12,  0, ...,  0,  0,  2], dtype=int64),\n",
              "  'sci.space': array([32, 92,  2, ...,  0,  0,  0], dtype=int64),\n",
              "  'talk.religion.misc': array([1, 9, 0, ..., 0, 0, 0], dtype=int64)})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_arr = xcv_train.toarray()\n",
        "\n",
        "words_per_class = {}\n",
        "word_freqs_per_class = {}\n",
        "\n",
        "for cls in indexes.keys():\n",
        "    class_idxs = indexes[cls] # нашли индексы строк матрицы, относящихся к классу cls\n",
        "\n",
        "    subarray_rows = x_arr[class_idxs] # нашли подмассив, относящийся к  классу cls\n",
        "    subarray_sum = np.sum(subarray_rows, axis = 0) # провели суммирование по столбцам\n",
        "    word_freqs_per_class[cls] = subarray_sum\n",
        "\n",
        "    words_per_class[cls] = len(subarray_sum[subarray_sum != 0]) # узнали,\n",
        "        # сколько слов было использовано в рамках одного класса, \n",
        "        # то есть просто подсчитали число ненулевых элементов\n",
        "\n",
        "words_per_class, word_freqs_per_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GSxOKIxLgn_"
      },
      "source": [
        "Все вышенаписанные переменные образуют метод `fit` будущего классификатора. Теперь внесите этот код в метод fit, и не забудьте сделать найденные переменные полями экземпляра класса при помощи `self`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "cHVzNkO8Lgn_"
      },
      "outputs": [],
      "source": [
        "class NaiveBayes:\n",
        "    def __init__(self):\n",
        "        # Инициализация полей\n",
        "        self.indexes = {}  # Словарь для хранения индексов строк по классам\n",
        "        self.words_per_class = {}  # Словарь для хранения количества уникальных слов по классам\n",
        "        self.word_freqs_per_class = {}  # Словарь для хранения частот слов по классам\n",
        "        self.vectorizer = None  # Векторизатор для преобразования текста в числовой формат\n",
        "        self.class_probs = {}  # Априорные вероятности классов\n",
        "\n",
        "    def fit(self, x_train, y_train):\n",
        "        \"\"\"\n",
        "        Метод для обучения классификатора.\n",
        "        \n",
        "        :param x_train: Текстовые данные для обучения (список строк).\n",
        "        :param y_train: Метки классов для обучения.\n",
        "        \"\"\"\n",
        "        # Создаём и обучаем CountVectorizer\n",
        "        self.vectorizer = CountVectorizer(stop_words='english')\n",
        "        x_arr = self.vectorizer.fit_transform(x_train).toarray()  # Преобразуем тексты в числовой формат\n",
        "\n",
        "        # Вычисляем априорные вероятности классов\n",
        "        total_samples = len(y_train)\n",
        "        unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
        "        for cls, count in zip(unique_classes, class_counts):\n",
        "            self.class_probs[cls] = count / total_samples\n",
        "\n",
        "        # Создаём словарь indexes\n",
        "        for cls in unique_classes:\n",
        "            class_idxs = np.where(y_train == cls)[0]  # Индексы строк, относящихся к классу cls\n",
        "            self.indexes[cls] = class_idxs\n",
        "\n",
        "            # Подсчёт частот слов и количества уникальных слов для каждого класса\n",
        "            subarray_rows = x_arr[class_idxs]  # Подмассив строк, относящихся к классу cls\n",
        "            subarray_sum = np.sum(subarray_rows, axis=0)  # Суммирование по столбцам\n",
        "            self.word_freqs_per_class[cls] = subarray_sum  # Частоты слов\n",
        "            self.words_per_class[cls] = len(subarray_sum[subarray_sum != 0])  # Количество уникальных слов\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        \"\"\"\n",
        "        Метод для предсказания классов на новых данных.\n",
        "        \n",
        "        :param x_test: Текстовые данные для предсказания (список строк).\n",
        "        :return: Предсказанные метки классов.\n",
        "        \"\"\"\n",
        "        # Преобразуем текстовые данные в числовой формат\n",
        "        x_test_arr = self.vectorizer.transform(x_test).toarray()\n",
        "\n",
        "        # Список для хранения предсказаний\n",
        "        predictions = []\n",
        "\n",
        "        # Проходим по каждому документу в тестовой выборке\n",
        "        for doc in x_test_arr:\n",
        "            class_scores = {}\n",
        "\n",
        "            # Вычисляем вероятность для каждого класса\n",
        "            for cls in self.indexes.keys():\n",
        "                # Логарифм априорной вероятности класса\n",
        "                log_prob = np.log(self.class_probs[cls])\n",
        "\n",
        "                # Логарифм правдоподобия (с учётом частот слов)\n",
        "                word_freqs = self.word_freqs_per_class[cls]\n",
        "                total_words_in_class = np.sum(word_freqs)\n",
        "                vocab_size = len(self.vectorizer.vocabulary_)\n",
        "\n",
        "                # Вычисляем логарифм правдоподобия с использованием сглаживания Лапласа\n",
        "                likelihood = np.sum(doc * np.log((word_freqs + 1) / (total_words_in_class + vocab_size)))\n",
        "\n",
        "                # Общий счёт для класса\n",
        "                class_scores[cls] = log_prob + likelihood\n",
        "\n",
        "            # Выбираем класс с максимальным счётом\n",
        "            predicted_class = max(class_scores, key=class_scores.get)\n",
        "            predictions.append(predicted_class)\n",
        "\n",
        "        return np.array(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4COT-aNDLgn_"
      },
      "source": [
        "Теперь реализуем метод `predict`. Вспомните еще раз [большую формулу](#clf-description) из начала этого раздела. Если вы внимательно читали статью, то заметили, что в примере мы получили не чистые вероятности классов, а всего лишь числовые оценки. Далее эти оценки можно перевести в вероятности, но мы этого делать не будем."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7dnSPbILgn_"
      },
      "source": [
        "Тогда промежуточный выход классификатора обозначим так:\n",
        "\n",
        "`pred_per_class = {<номер строки в тестовой выборке>: {<класс 1>: <оценка>, ... , <класс n>: <оценка>}}`\n",
        "\n",
        "Таким образом итоговый класс к которому будет отнесена строка - просто класс с наибольшей оценкой"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo7EWqLHLgoA"
      },
      "source": [
        "Не забудьте о следующих вещах:\n",
        "- X_test - это такая же разряженная матрица, нужно превратить ее в список\n",
        "- переменная $\\frac{D_c}{D}$ одинакова для одного класса.\n",
        "- Чтобы подсчитать $W_{ic}$ воспользуйтесь `word_freqs_per_class`.\n",
        "- Чтобы понять, какие элементы вам нужно брать в `word_freqs_per_class`, найдите индексы ненулевых элементов в классифицируемой строке - если эти элементы ненулевые, значит, там было какое-то слово\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iws9JGvsdoHl"
      },
      "source": [
        "**Hints:**\n",
        "- [np.nonzero](https://numpy.org/doc/stable/reference/generated/numpy.nonzero.html) (будьте внимательны с типом возвращаемого значения!)\n",
        "- [Math.log](https://docs.python.org/3/library/math.html) для чисел\n",
        "- [np.log](https://numpy.org/doc/stable/reference/generated/numpy.log.html) для массивов\n",
        "- Так как с каждым разом вы добавляете оценку для нового класса, логично использовать [defaultdict(dict)](https://docs.python.org/3/library/collections.html#collections.defaultdict): так создается словарь, состоящий из пустых словарей, и при обращении к элементу словаря, например `pred_per_class['SomeID']` мы получаем словарь, для которого доступны все стандартные методы (например, update)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "vvuxVe5TLgoA"
      },
      "outputs": [],
      "source": [
        "# Пример использования\n",
        "classifier = NaiveBayes()\n",
        "classifier.fit(x_train_cleaned, y_train_cleaned)\n",
        "\n",
        "# Предсказание на тестовых данных\n",
        "y_pred = classifier.predict(x_test_cleaned)\n",
        "\n",
        "# Проверка размерности предсказаний\n",
        "assert len(y_pred) == len(y_test_cleaned), \"Размерность предсказаний не совпадает с размерностью y_test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJyBqIc7fo10"
      },
      "source": [
        "### Проверка классификатора\n",
        "\n",
        "Воспользуемся матрицей ошибок и отчетом классификации из `sklearn`, проверим классификатор и на count_vectorizer-векторах, и на tf-idf-векторах. Для начала поговорим о способах оценки качества классификации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPpD5E193W0B"
      },
      "source": [
        "#### Матрица ошибок"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4EgZ5ow3b3Q"
      },
      "source": [
        "Основной материал можете прочесть в [статье](https://habr.com/ru/company/ods/blog/328372/), тут же напишем кратко."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osbSv3zY3xDU"
      },
      "source": [
        "Рассмотрим матрицу: \n",
        " \n",
        " \n",
        " <br></br> | $y=1$ | $y=0$\n",
        "--- | --- | ---\n",
        "$\\overline{y} = 1$ | **True Positive (TP)** | **False Positive (FP)**\n",
        "$\\overline{y} = 0$ | **False Negative (FN)** | **True Negative (TN)**\n",
        "\n",
        "$y$ - истинная метка класса, $\\overline{y}$ - предсказание классификатора.\n",
        "\n",
        "По главной диагонали матрицы - число правильно классифицированных объектов (TP и FP).  \n",
        "\n",
        "Понятно, что число классов может быть больше двух, но для простоты будем рассматривать только случай с двумя классами, назовем их условно Positive и Negative.\n",
        "\n",
        "Ошибки бывают двух типов: **ошибки первого рода** (FP), или ложноположительное срабатывание, когда, например, анализ показывает заболевание, хотя на самом деле человек здоров, и **ошибки второго рода** (FN), или пропуск события, когда больного человека принимают за здорового. \n",
        "\n",
        "В некотрых случаях по оси X может отображаться предсказание модели, а по оси Y - истинные метки. Важно не запоминать положение элементов в матрице, а в зависимости от заданных осей понимать, где ошибки, а где правильные значения.\n",
        "\n",
        "**Метрики**\n",
        "\n",
        "_Accuracy_, или точность.  \n",
        "Обозначает в целом долю правильных ответов:\n",
        "\n",
        "$$\n",
        "Accuracy = \\frac{TP + TN}{TP + FP + TN + FN}\n",
        "$$\n",
        "\n",
        "Недостаток метрики в том, что она не работает на несбалансированных выборках.\n",
        "\n",
        "_Precision_ и _Recall_, или точность и полнота.  \n",
        "\n",
        "---\n",
        "\n",
        "**Важное замечание**  \n",
        "_Оба термина переводятся на русский язык как точность, хотя при этом отражают разные понятия. Чтобы не допустить недопонимая, можно использовать английские слова напрямую, либо accuracy называть аккуратностью, а precision - точностью. Полнота - она и есть полнота._\n",
        "\n",
        "---\n",
        "\n",
        "Эти две метрики рассчитываются отдельного для каждого из классов. Для примера рассмотрим класс Positive:\n",
        "\n",
        "$$\n",
        "Precision = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "$$\n",
        "Recall = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "Precision отражает долю объектов класса Positive, которые классификатор классифицировал верно.\n",
        "\n",
        "Recall показывает долю объектов класса Positive среди всех объектов класса Positive, которые вообще нашел алгоритм.\n",
        "\n",
        "F-мера - один из способов объеденить Precision и Recall:\n",
        "\n",
        "$$\n",
        "F_\\beta = (1 + \\beta^2) \\cdot \\frac{Precision \\cdot Recall}{\\beta^2 \\cdot Precision + Recall}\n",
        "$$\n",
        "\n",
        "$\\beta$ - вес Precision в метрике.\n",
        "\n",
        "В `sklearn` за матрицу ошибок и метрики отвечают [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) и [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BKyh41ZLgoB",
        "outputId": "4f24de4d-de62-4acc-b057-b895262b16cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.73      0.69       311\n",
            "           1       0.91      0.92      0.91       384\n",
            "           2       0.86      0.87      0.86       378\n",
            "           3       0.66      0.55      0.60       245\n",
            "\n",
            "    accuracy                           0.79      1318\n",
            "   macro avg       0.77      0.76      0.77      1318\n",
            "weighted avg       0.79      0.79      0.79      1318\n",
            "\n",
            "[[226   7  20  58]\n",
            " [ 11 352  18   3]\n",
            " [ 24  19 328   7]\n",
            " [ 87   8  16 134]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Создаём экземпляр классификатора\n",
        "nb = NaiveBayes()\n",
        "\n",
        "# Обучаем модель на текстовых данных\n",
        "nb.fit(x_train_cleaned, y_train_cleaned)  # Передаём текстовые данные, а не xcv_train\n",
        "\n",
        "# Предсказываем классы для тестовых данных\n",
        "pred = nb.predict(x_test_cleaned)  # Передаём текстовые данные, а не xcv_test\n",
        "\n",
        "# Выводим отчёт о классификации\n",
        "print(classification_report(y_test_cleaned, pred))\n",
        "print(confusion_matrix(y_test_cleaned, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZr0BWzTFm71"
      },
      "source": [
        "Проверим классификатор на полученных ранее tf-idf векторах"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYA5YltjLgoB",
        "outputId": "574418c8-69ce-4e4c-e2fe-450218119ac9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.73      0.69       311\n",
            "           1       0.91      0.92      0.91       384\n",
            "           2       0.86      0.87      0.86       378\n",
            "           3       0.66      0.55      0.60       245\n",
            "\n",
            "    accuracy                           0.79      1318\n",
            "   macro avg       0.77      0.76      0.77      1318\n",
            "weighted avg       0.79      0.79      0.79      1318\n",
            "\n",
            "[[226   7  20  58]\n",
            " [ 11 352  18   3]\n",
            " [ 24  19 328   7]\n",
            " [ 87   8  16 134]]\n"
          ]
        }
      ],
      "source": [
        "# Создаём экземпляр классификатора\n",
        "nb = NaiveBayes()\n",
        "\n",
        "# Обучаем модель на текстовых данных\n",
        "nb.fit(x_train_cleaned, y_train_cleaned)  # Используем очищенные данные\n",
        "\n",
        "# Предсказываем классы для тестовых данных\n",
        "pred = nb.predict(x_test_cleaned)  # Используем очищенные данные\n",
        "\n",
        "# Выводим отчёт о классификации\n",
        "print(classification_report(y_test_cleaned, pred))\n",
        "print(confusion_matrix(y_test_cleaned, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV-8MaAjNEGn"
      },
      "source": [
        "### Сравним с версией из sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер x_train после очистки: 1977\n",
            "Размер y_train после очистки: 1977\n",
            "Размер y_train после очистки: 2034\n",
            "Размер x_train после очистки: 1977\n"
          ]
        }
      ],
      "source": [
        "print(f\"Размер x_train после очистки: {len(x_train_cleaned)}\")\n",
        "print(f\"Размер y_train после очистки: {len(y_train_cleaned)}\")\n",
        "print(f\"Размер y_train после очистки: {len(y_train)}\")\n",
        "print(f\"Размер x_train после очистки: {len(x_arr)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcEkWenYLgoC",
        "outputId": "a01e8f7b-e26d-4948-9363-7d039edb18dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.75      0.68       311\n",
            "           1       0.91      0.91      0.91       384\n",
            "           2       0.83      0.89      0.86       378\n",
            "           3       0.68      0.42      0.52       245\n",
            "\n",
            "    accuracy                           0.78      1318\n",
            "   macro avg       0.76      0.74      0.74      1318\n",
            "weighted avg       0.78      0.78      0.77      1318\n",
            "\n",
            "[[232   9  28  42]\n",
            " [ 12 351  20   1]\n",
            " [ 20  18 335   5]\n",
            " [112   9  20 104]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf = MultinomialNB(alpha=4)\n",
        "clf.fit(xcv_train, y_train_cleaned)\n",
        "\n",
        "y_pred = clf.predict(xcv_test)\n",
        "print(classification_report(y_test_cleaned, y_pred))\n",
        "\n",
        "print(confusion_matrix(y_test_cleaned, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCo-Zg6aNBFY",
        "outputId": "63f081d7-7315-4b28-89a1-5179f98a1dbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.64      0.62       311\n",
            "           1       0.84      0.93      0.88       384\n",
            "           2       0.63      0.92      0.75       378\n",
            "           3       0.85      0.04      0.09       245\n",
            "\n",
            "    accuracy                           0.69      1318\n",
            "   macro avg       0.73      0.63      0.58      1318\n",
            "weighted avg       0.73      0.69      0.63      1318\n",
            "\n",
            "[[199  22  88   2]\n",
            " [  1 356  27   0]\n",
            " [  5  24 349   0]\n",
            " [126  20  88  11]]\n"
          ]
        }
      ],
      "source": [
        "clf = MultinomialNB(alpha=4)\n",
        "clf.fit(xTfidf_train, y_train_cleaned)\n",
        "\n",
        "y_pred = clf.predict(xTfidf_test)\n",
        "print(classification_report(y_test_cleaned, y_pred))\n",
        "\n",
        "print(confusion_matrix(y_test_cleaned, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPQbSuomieol"
      },
      "source": [
        "### Порассуждаем (дополнительно)\n",
        "\n",
        "Как вы заметили, качество предсказаний неоднородно (какие-то классы определяются хорошо, какие-то не очень хорошо). В чем может быть причина этого? \n",
        "\n",
        "Самостоятельно попробуйте найти топ-10 наиболее часто встречающихся слов в каждой категории, посмотрите на эти слова и напишите свои идеи, почему же при классификации точность падает на той или иной категории."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_2z_s2-jq_f"
      },
      "source": [
        "**_Поле для ответа_**\n",
        "\n",
        " . . ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1NzFNBMjpp3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G1JWVQjjpcp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpUW4_DCjpYo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psY_MzZKjpU6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NAIVE_BAYES_CG_UNSOLVED.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
